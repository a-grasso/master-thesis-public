"Zeitstempel","do you know your services usage patterns? how do they look like?","how easy was it to switch?","did you encounter roadblocks on the way?","could you solve them and if so how?","if, what roadblocks could not be solved (yet)?","did you notice any different behavior in your system?","how important is latency for your service and did you notice any losses in latency after switching?","did you look into alternatives other than LWA?","how happy are you with the switch?","Anything to add? :D"
"2024/09/03 1:06:00 PM OESZ","There are more requests during the day and very few during the night. On week-ends and during vacation times there are also fewer requests. (see attachment in chat)","5/10. Not too difficult as the existing application was already quite stateless in nature. The lambda web adapter was easy to integrate with the existing docker images. The deployment had to be adjusted to push the new image to lambda.","- Environment variables and secrets are different in Lambda from Kubernetes
- Internal service dependencies running in Kubernetes had to be removed
- NestJS internals that were not 100% stateless had to be removed (internal eventing, scheduling)
- Postgres connection limits are less controllable in serverless environment due to potentially unbound scaling of instances/client connections
- Database migration best practices
- Monitoring best practices
- Long cold starts","- Lambda environment variable size limit could be circumvented by embedding a ""static"" CA certificate inside the docker image
- Kubernetes secret env vars were simply put into lambda environment variables without using AWS Secret Management as this would require adjustments to the source code. The secrets are still encrypted at rest inside AWS but visible in the AWS management console.
- Internal event handlers and scheduling could be removed from the lambda application. There's a separate deployment in the existing Kubernetes environment that contains the scheduling functionality. This can be migrated to separate scheduled lambda deployments in the future. This was only possible because the existing Project K codebase already had the capability to run with different profiles to allow a ""scheduling-only"" deployment which can continue to run in the Kubernetes environment.
- Cold starts could be reduced from around 10s to around 3s by reducing the docker image size by making use of Webpack bundling. Also a dedicated NestJS module was introduced with fewer dependencies to further reduce the bundle size.","- Postgres connection limits are expected to be hit if the Lambda load increases drastically. Currently the average load on the application is not as high, so it's possible to use a dedicated Postgres connection pool with no issues. If there is a sudden burst of load that creates parallel Lambda instances, too many connections will be opened against the Postgres which will result in failure at runtime. The simplest solution for this is to either scale up the Postgres instance to allow for more concurrent connections (high cost, does not scale with load) or to use a ""serverless"" Postgres offering like CockroachDB or Neon which advertises more dynamic connection management.
- Kubernetes init container migration best practices were used prior. Now the migration scripts will have to be moved to a ""deployment"" script. Best practices need to be researched.
- Monitoring was based on Prometheus/Grafana. Since Lambdas can not be scraped by Prometheus as they do not persist over time, a different solution will need to be found. Probably heavier vendor lock-in will be happen when using integrated Cloudwatch tooling. Currently there is less observability of the API Gateway + Lambda integration. Logs need to be monitored to find possible errors.","- Some cold start delay can be observed when using the web application as the NestJS cold start invocation takes around three seconds. Cloudwatch metrics also show the cold starts quite well.","There are no hard requirements for latency or SLA/SLOs. There's a personal goal of staying below 1s for every request, ideally around 200ms per request. Previously slow requests were caused mostly by unoptimized database queries which could be improved. Currently the slow cold starts can occur on any request.","- I considered deploying the NestJS/Node application as a ""normal"" zip deployment. My initial attempts failed because the resulting package exceeded the 50MB. According to my latest knowledge this should work if using my latest Webpack configuration where the package size seems to be around 2MB.","4","The above judgement of the switch is currently only based on the fact that the switch was possible and seems promising in terms of costs. Since I haven't run this for long enough in production, I will update my answer later."